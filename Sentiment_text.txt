Sentiment_text

SENTIMENT ANALYSIS USING NAIVE BAYES 

Sentiment analysis is an important task in natural language processing (NLP) that involves determining the emotional tone behind a text. This subcategory of text mining can be used for many applications, such as reputation management, where the general sentiment behind a brand or a newly implemented marketing strategy is analysed, helping to determine the public response to the changes and to offer insights into what the next steps of implementation would be, based on a positive or negative response. 

Naive Bayes
For our project we decided to implement the machine learning-based Naive Bayes classifier for sentence-level sentiment analysis due to its simplicity, efficiency and proven effectiveness in classification tasks. Naive Bayes is based on the Bayes’ Theorem that is used to calculate the probability of a sentiment label (positive, neutral or negative) given the set of words in the sentence. Despite the “naive” assumption of feature independence, this model performs well in text classification. One of the reasons it does well is represented by the fact that the model leverages word frequencies to estimate sentiment probabilities and that the probabilistic output allows for confidence scoring, which is useful in uncertain cases. Another reason behind our choices is that the model has high interpretability, meaning that it is easier to analyze why certain predictions are made, making the process of debugging and refinement smoother. 

Furthermore, sentiment analysis involves processing large datasets, meaning that computational efficiency is crucial. In our project, the training data we used includes 36.000 instances, as we tried to reproduce a real life scenario of implementation. This is a pretty standard number of instances, therefore, the problem of computational efficiency is apparent. Keeping this in mind, the choice of doing sentiment analysis using a Naive Bayes model is further justified by the fact that it requires a single pass through the training data in order to compute the word probabilities, making it faster to train compared to more complex models such as neural networks. Its simplicity represents an advantage in our case, as it performs well even with limited computational resources, which is beneficial for research projects with constraints. Additionally, due to its simple architecture, it has a lower risk of overfitting on the training data provided, making it a generalizable model to use above multiple datasets. 

Fine Tuning
In order to find which values of initialization of the model are best suited for our project, we implemented a grid search algorithm tasked with hyperparameter fine-tuning. The parameters we decided to look at are the following: minimum document frequency, which is indicative of the number of times a specific token will be taken into account (with values of two, five or ten), range of n-grams, which controls the range of contiguous sequences of n words included in the feature set (with values of (1,1), (1,2) or (2,2)), whether to use Bag of Words representation or TF-IDF representation (with boolean values of either True or False) and alpha, representing the smoothing factor of the Naive Bayes model (with values of 0.001, 0.01, 0.1 or 1.0). In order to evaluate the models with the sets of hyperparameters, 5-fold crossover was implemented, meaning that we divided our training data into five parts, using one part as validation data. Iteratively, each model with a different set of values is trained and tested on the five different versions of the training and validation datasets and then the macro F1-score and the weighted F1-score are used as evaluation functions. Based on these values, the best performing model was picked with the following set of parameters: minimum document frequency of 2, n-gram range of (1, 2), use of TF-IDF with value True and smoothing factor of 0.1. 

ANALYSIS 

Best found solution
The performance of the grid search found model is not exceptionally good, as it reaches an accuracy of 50%, meaning that it can only identify the right sentiment in half of the cases given to it. As a comparison of performance we picked another set of parameters to test when initializing the Naive Bayes model. The choice of parameters was based on previous experience we had gathered with the architecture, for the same task of sentiment analysis, when completing one of the assignments of the Text Mining Course, offered by Vrije Universiteit [1]. The chosen set of values (minimum document frequency of 5, n-gram range set on its default value, with use of TF-IDF of False, meaning that the representation of words and tokens is Bag of Words and smoothing factor left on its default value) has been analysed to be the best one performing the task of sentiment analysis given a refined dataset of airline tweets. For this reason, we decided to use it as a control measure, to check whether the grid search algorithm will find a better performing solution. The outcome of this comparison was unexpected, as the second set of values found for initializing the Naive Bayes model ended up outperforming the best solution found by hyperparameter tuning by 16%, meaning that it reached an accuracy of 66.7%. By comparing these two different behaviours, an observed insight is that our training data is not as generalizable or robust as we would like it to be, meaning that even if we have an equal distribution of domain-related text included inside of it, it is still not enough to make it representative for a real life scenario of implementation. 

Motivating the performances
Furthermore, the difference in behaviour between the two sets of values that Naive Bayes was initialized with can be attributed to the fact that the first configuration allowed for very rare terms (appearing in only two documents as defined by the minimum document frequency) to be included as features. These terms can be noisy, domain-specific or outliers, which ends up hurting generalization. As opposed to this method, the second configuration filters out the rare terms by keeping those that appear in at least five documents, reducing noise and focusing on more stable, frequent terms that are likely more meaningful for sentiment.

A second difference can be represented by the set n-gram range. For the first configuration the range (1,2) includes unigrams and bigrams, meaning that while bigrams capture phrases like “not good” or “very good”, which are useful for sentiment, they might also introduce many low-level or irrelevant combinations, such as “the movie” or “the book”, increasing dimensionality and noise. As compared to the first model, the second model uses the default range (1,1), making it use only unigrams. This approach may miss some meaningful phrases, but unigrams often prove to be sufficient for sentiment analysis, also helping the configuration avoid noise and simplifying the feature space, reducing overfitting. 

Another very important difference is represented by the method of representation used between the two models. On one hand, the first configuration uses TF-IDF to represent its words, which downweights frequent terms, such as “the”, and emphasizes rare but discriminative terms. This method isn’t ideal for sentiment analysis, as common words, such as “love” or “hate”, are highly informative and TF-IDF’s downweighting can dilute their importance. On the other hand, the second configuration uses Bag of Words representation, which preserves the raw term frequencies, meaning that it can be more effective for sentiment tasks, as just the count of sentiment words is directly correlated with the sentiment strength. 

Lastly, the smoothing factor can have a big impact on the performance of the two models, as the first one uses a smaller alpha of 0.1, that provides less smoothing, making the model more sensitive to the training, while the second one uses the default alpha value of 1.0, which represents the default Laplace smoothing, being more conservative and helping handle unseen words better. Using Laplace smoothing improves generalization by avoiding extreme probabilities for rare terms. 
